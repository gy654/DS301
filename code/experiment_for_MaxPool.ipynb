{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for neural style transfer using AlexNet and VGG16 (MaxPool)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms , models \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Grace Directory\n",
    "# ! cp -r 'drive/MyDrive/ds301HW/neural style transfer' '/content'\n",
    "\n",
    "# Stephen Directory\n",
    "# ! cp -r 'drive/MyDrive/DS-UA 301/DS301' '/content' \n",
    "\n",
    "# Feature Extraction on each layer\n",
    "def model_activations(input,model): \n",
    "  global layers\n",
    "\n",
    "  features = {}\n",
    "  x = input\n",
    "  x = x.unsqueeze(0)\n",
    "  for name,layer in model._modules.items():\n",
    "      x = layer(x)\n",
    "      if name in layers:\n",
    "          features[layers[name]] = x \n",
    "  return features\n",
    "\n",
    "  # Preprocess image\n",
    "def imcnvt(image):\n",
    "    x = image.to(\"cpu\").clone().detach().numpy().squeeze()\n",
    "    x = x.transpose(1,2,0)\n",
    "    x = x*np.array((0.5,0.5,0.5)) + np.array((0.5,0.5,0.5))\n",
    "    return np.clip(x,0,1)\n",
    "\n",
    "# Helper function on sytle loss\n",
    "def gram_matrix(imgfeature):\n",
    "    _,d,h,w = imgfeature.size()\n",
    "    imgfeature = imgfeature.view(d,h*w)\n",
    "    gram_mat = torch.mm(imgfeature,imgfeature.t())\n",
    "    return gram_mat\n",
    "\n",
    "# Load and preprocess and plot the image\n",
    "def preprocess_and_plot(plot = True):\n",
    "  global content_path, style_path\n",
    "  transform = transforms.Compose([transforms.Resize(300),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "  content = Image.open(content_path).convert(\"RGB\")\n",
    "  content = transform(content).to(device)\n",
    "  print(\"Content shape => \", content.shape)\n",
    "  style = Image.open(style_path).convert(\"RGB\")\n",
    "  style = transform(style).to(device)\n",
    "\n",
    "  if plot:\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "    ax1.imshow(imcnvt(content),label = \"Content\")\n",
    "    plt.axis('off')\n",
    "    ax2.imshow(imcnvt(style),label = \"Style\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "  target = content.clone().requires_grad_(True).to(device)\n",
    "  return content, style, target\n",
    "\n",
    "# Feature Extraction for both sytle and content loss\n",
    "def extract_features(content, style):\n",
    "  global model, layers\n",
    "  style_features = model_activations(style,model)\n",
    "  content_features = model_activations(content,model)\n",
    "  style_grams = {layer:gram_matrix(style_features[layer]) for layer in style_features}\n",
    "  return content_features, style_features, style_grams\n",
    "\n",
    "# Trianing loop for the experiment\n",
    "def training_loop(content_features, target, style_grams, epochs, print_after, aggregate = True):\n",
    "\n",
    "  global content_index, style_index\n",
    "  optimizer = torch.optim.Adam([target],lr=0.007)\n",
    "\n",
    "  for i in range(1,epochs+1):\n",
    "\n",
    "    target_features = model_activations(target,model)\n",
    "\n",
    "    if aggregate:\n",
    "\n",
    "      content_loss = 0\n",
    "      for layer in content_features.keys():\n",
    "        content_loss = torch.mean((content_features[layer]-target_features[layer])**2)\n",
    "        content_loss += content_loss\n",
    "\n",
    "      content_loss = content_loss.mean()\n",
    "\n",
    "    else:\n",
    "      \n",
    "      content_loss = torch.mean((content_features[list(content_features.keys())[-2]]-target_features[list(target_features.keys())[-2]])**2) \n",
    "\n",
    "\n",
    "    style_loss = 0\n",
    "    for layer in style_wt_meas:\n",
    "\n",
    "        style_gram = style_grams[layer]\n",
    "        target_gram = target_features[layer]\n",
    "        _,d,w,h = target_gram.shape\n",
    "        target_gram = gram_matrix(target_gram)\n",
    "        style_loss += (style_wt_meas[layer]*torch.mean((target_gram-style_gram)**2))/d*w*h\n",
    "    \n",
    "    total_loss = content_wt*content_loss + style_wt*style_loss\n",
    "    \n",
    "    if i%1000==0:       \n",
    "        print(\"epoch \",i,\" \", total_loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%print_after == 0:\n",
    "        plt.imshow(imcnvt(target),label=\"Epoch \"+str(i))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.imsave(f'vgg{content_index}-{style_index}.png',imcnvt(target),format='png')\n",
    "\n",
    "# Main function for the experiment\n",
    "def main(content_wt, epoch,print_after, plot= False, aggregate = True):\n",
    "  content, style, target = preprocess_and_plot(plot = plot)\n",
    "  content_features, style_features, style_grams = extract_features(content, style)\n",
    "  training_loop(content_features, target, style_grams, epochs = epoch, print_after= print_after, aggregate = aggregate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image paths for the experiment\n",
    "content_index, style_index = 3, 5 # Change the index to change the image\n",
    "\n",
    "# Grace Directory\n",
    "# content_path = f\"neural style transfer/images/{content_index}.jpeg\"\n",
    "# style_path = f\"neural style transfer/style/{style_index}.jpeg\"\n",
    "\n",
    "# Stephen Directory\n",
    "# content_path = f\"DS301/images/{content_index}.jpeg\"\n",
    "# style_path = f\"DS301/style/{style_index}.jpeg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for the neural style transfer using VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments for the VGG19 model\n",
    "\n",
    "def selectLayers(layerALL = True):\n",
    "\n",
    "    if layerALL == False:\n",
    "\n",
    "        # Selecting the layers for the experiment\n",
    "        layers = {\n",
    "            '4': 'maxpool_4',\n",
    "            '18': 'maxpool_18',\n",
    "            '36': 'maxpool_36'\n",
    "        }\n",
    "        # Selecting the weights for the experiment\n",
    "        style_wt_meas = {\n",
    "                    \"maxpool_4\": 1.0,\n",
    "                    \"maxpool_18\": 0.4,\n",
    "                    \"maxpool_36\": 0.1\n",
    "                }\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Selecting the layers for the experiment\n",
    "        layers = {\n",
    "            '4': 'maxpool_4',\n",
    "            '9': 'maxpool_9',\n",
    "            '18': 'maxpool_18',\n",
    "            '27': 'maxpool_27',\n",
    "            '36': 'maxpool_36'\n",
    "        }\n",
    "        # Selecting the weights for the experiment\n",
    "        style_wt_meas = {\n",
    "                    \"maxpool_4\": 1.0,\n",
    "                    \"maxpool_9\": 0.8,\n",
    "                    \"maxpool_18\": 0.4,\n",
    "                    \"maxpool_27\": 0.2,\n",
    "                    \"maxpool_36\": 0.1\n",
    "                }\n",
    "\n",
    "    return layers, style_wt_meas\n",
    "\n",
    "# Whehter select all the layers for the experiment\n",
    "layerALL = True\n",
    "layers, style_wt_meas = selectLayers(layerALL = layerALL)\n",
    "\n",
    "# Loading the pretrained VGG16 model\n",
    "model = models.vgg19(pretrained= True).features\n",
    "\n",
    "# Setting the content and style weights\n",
    "content_wt = 100\n",
    "style_wt = 1e4\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "\n",
    "content_wts = 100\n",
    "style_wt = 10000\n",
    "epoch = 100000\n",
    "print_after = 1000\n",
    "\n",
    "\n",
    "# Running the experiment for the VGG16 model\n",
    "main(content_wt, epoch = epoch, print_after = print_after, plot = True, aggregate = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for the neural style transfer using AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments for the AlexNet model\n",
    "\n",
    "def selectLayers(layerALL = True):\n",
    "\n",
    "    if layerALL == False:\n",
    "\n",
    "        # Selecting the layers for the experiment\n",
    "        layers = {\n",
    "            '2' : 'maxpool_2',\n",
    "            '12': 'maxpool_12'\n",
    "        }\n",
    "        # Selecting the weights for the experiment\n",
    "        style_wt_meas = {\n",
    "                    \"maxpool_2\": 1.0,\n",
    "                    \"maxpool_12\": 0.1\n",
    "                }\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Selecting the layers for the experiment\n",
    "        layers = {\n",
    "            '2' : 'maxpool_2',\n",
    "            '5': 'maxpool_5',\n",
    "            '12': 'maxpool_12'\n",
    "        }\n",
    "        # Selecting the weights for the experiment\n",
    "        style_wt_meas = {\n",
    "                    \"maxpool_2\": 1.0,\n",
    "                    \"maxpool_5\": 0.5,\n",
    "                    \"maxpool_12\": 0.1\n",
    "                }\n",
    "\n",
    "    return layers, style_wt_meas\n",
    "\n",
    "# Whehter select all the layers for the experiment\n",
    "layerALL = True\n",
    "layers, style_wt_meas = selectLayers(layerALL = layerALL)\n",
    "\n",
    "# Loading the pretrained VGG16 model\n",
    "model = models.alexnet(pretrained= True).features\n",
    "\n",
    "# Setting the content and style weights\n",
    "content_wt = 100\n",
    "style_wt = 1e4\n",
    "\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)\n",
    "\n",
    "content_wts = 100\n",
    "style_wt = 10000\n",
    "epoch = 100000\n",
    "print_after = 1000\n",
    "\n",
    "\n",
    "# Running the experiment for the VGG16 model\n",
    "main(content_wt, epoch = epoch, print_after = print_after, plot = True, aggregate = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:28:27) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
